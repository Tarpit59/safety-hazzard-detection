{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply model on images and store it with 4 classes(Vest, NOVest, Helmet, NOHelmet)\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import os\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "def draw_text_and_box_on_image(image, text, position, box_coordinates, font, color):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    # Draw bounding box\n",
    "    draw.rectangle(box_coordinates, outline=color, width=2)\n",
    "    \n",
    "    # Draw text\n",
    "    draw.text(position, text, fill=color, font=font)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def process_images(folder_path, model, output_folder):\n",
    "    # List all files in the specified folder\n",
    "    files = os.listdir(folder_path)\n",
    "\n",
    "    # Filter only image files (you can customize this based on your image file extensions)\n",
    "    image_files = [f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]\n",
    "    \n",
    "    for image_file in image_files:\n",
    "        # Construct the full path to the image\n",
    "        image_path = os.path.join(folder_path, image_file)\n",
    "        # Open the image using Pillow\n",
    "        try:\n",
    "            results = model.predict(image_path)\n",
    "            result = results[0]\n",
    "            # Create a font with the desired size\n",
    "            font_size = 10  # Adjust the font size as needed\n",
    "            font = ImageFont.truetype(\"arial.ttf\", font_size)  # Use arial.ttf or another font file with the desired size\n",
    "            original_image = Image.open(image_path)\n",
    "            \n",
    "            for idx, box in enumerate(result.boxes):\n",
    "                class_id = result.names[box.cls[0].item()]\n",
    "                cords = box.xyxy[0].tolist()\n",
    "                cords = [round(x) for x in cords]\n",
    "                conf = round(box.conf[0].item(), 1)\n",
    "                \n",
    "                # Determine color and text based on class_id\n",
    "                if class_id == \"Helmet\":\n",
    "                    bounding_box_color = (0, 255, 0)  # Green\n",
    "                    text_color = (0, 255, 0)  # Green\n",
    "                elif class_id == \"NOHelmet\":\n",
    "                    bounding_box_color = (0, 0, 255)  # Blue\n",
    "                    text_color = (0, 0, 255)  # Blue\n",
    "                elif class_id == \"NOVest\":\n",
    "                    bounding_box_color = (255, 0, 0)  # Red\n",
    "                    text_color = (255, 0, 0)  # Red\n",
    "                elif class_id == \"Vest\":\n",
    "                    bounding_box_color = (255, 255, 0)  # Yellow\n",
    "                    text_color = (255, 255, 0)  # Yellow\n",
    "                else:\n",
    "                    # Default color for unknown classes\n",
    "                    bounding_box_color = (128, 128, 128)  # Gray\n",
    "                    text_color = (128, 128, 128)  # Gray\n",
    "\n",
    "                # Draw text and bounding box on the image\n",
    "                text = f\"{class_id}({conf})\"\n",
    "                position = (cords[0], cords[1] - 12)  # Adjust the position based on your preference\n",
    "                image_with_text_and_box = draw_text_and_box_on_image(original_image, text, position, cords, font, bounding_box_color)\n",
    "                \n",
    "            # Save the modified image to the output folder outside the loop\n",
    "            output_path = os.path.join(output_folder, f'modified_image_{image_file.replace(\".\", \"_\")}.png')  # Use a unique identifier for each image\n",
    "            image_with_text_and_box.save(output_path, format='PNG')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image_path}: {e}\")\n",
    "\n",
    "# Example usage\n",
    "output_folder = \"Your output folder path\"\n",
    "model = YOLO(\"Your Model path\")\n",
    "folder_path = \"Your Input folder path\"\n",
    "process_images(folder_path, model, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to apply model on video with 4 classes(Vest, NOVest, Helmet, NOHelmet)\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def draw_text_and_box_on_image(image, text, position, box_coordinates, font, color):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    # Draw bounding box\n",
    "    draw.rectangle(box_coordinates, outline=color, width=2)\n",
    "    \n",
    "    # Draw text with specified color\n",
    "    draw.text(position, text, fill=color, font=font)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def process_video(video_path, model, output_folder, output_video_path):\n",
    "    # Open the video capture\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Get the frames per second (fps) of the input video\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Create output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Create VideoWriter object with the same fps as the input video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (int(cap.get(3)), int(cap.get(4))))\n",
    "\n",
    "    frame_number = 0\n",
    "\n",
    "    while True:\n",
    "        # Read a frame from the video\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Break the loop if the video has ended\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert the OpenCV BGR image to RGB (PIL format)\n",
    "        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        try:\n",
    "            results = model.predict(image)\n",
    "            result = results[0]\n",
    "\n",
    "            # Create a font with the desired size\n",
    "            font_size = 20  # Adjust the font size as needed\n",
    "            font = ImageFont.truetype(\"arial.ttf\", font_size)  # Use arial.ttf or another font file with the desired size\n",
    "\n",
    "            # Keep track of whether any detection occurred in this frame\n",
    "            detection_occurred = False\n",
    "\n",
    "            for idx, box in enumerate(result.boxes):\n",
    "                class_id = result.names[box.cls[0].item()]\n",
    "                cords = box.xyxy[0].tolist()\n",
    "                cords = [round(x) for x in cords]\n",
    "                conf = round(box.conf[0].item(), 2)\n",
    "\n",
    "                # Determine color based on class_id\n",
    "                if class_id == \"Helmet\":\n",
    "                    bounding_box_color = (0, 255, 0)  # Green\n",
    "                    text_color = (0, 255, 0)  # Green\n",
    "                elif class_id == \"NOHelmet\":\n",
    "                    bounding_box_color = (0, 0, 255)  # Blue\n",
    "                    text_color = (0, 0, 255)  # Blue\n",
    "                elif class_id == \"NOVest\":\n",
    "                    bounding_box_color = (255, 0, 0)  # Red\n",
    "                    text_color = (255, 0, 0)  # Red\n",
    "                elif class_id == \"Vest\":\n",
    "                    bounding_box_color = (255, 255, 0)  # Yellow\n",
    "                    text_color = (255, 255, 0)  # Yellow\n",
    "                else:\n",
    "                    bounding_box_color = (128, 128, 128)  # Default to gray\n",
    "                    text_color = (128, 128, 128)  # Default to gray\n",
    "\n",
    "                # Draw text and bounding box on the image\n",
    "                text = f\"{class_id}({conf})\"\n",
    "                position = (cords[0], cords[1] - 22)  # Adjust the position based on your preference\n",
    "                image_with_text_and_box = draw_text_and_box_on_image(image, text, position, cords, font, text_color)\n",
    "\n",
    "                # Convert the modified image back to BGR (OpenCV format)\n",
    "                modified_frame = cv2.cvtColor(np.array(image_with_text_and_box), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                # If at least one detection occurred, set detection_occurred to True\n",
    "                if not detection_occurred:\n",
    "                    detection_occurred = True\n",
    "\n",
    "            # Write the frame to the output video (outside the detection loop)\n",
    "            out.write(modified_frame) if detection_occurred else out.write(frame)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing frame {frame_number}: {e}\")\n",
    "\n",
    "        frame_number += 1\n",
    "\n",
    "    # Release the video capture and writer\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "# Example usage\n",
    "output_folder = \"Your output folder path\"\n",
    "output_video_path = \"Your output video path\"\n",
    "model = YOLO(\"Your Model path\")\n",
    "video_path = \"Your Input video path\"\n",
    "process_video(video_path, model, output_folder, output_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to apply on webcam and show output for 4 classes(Vest, NOVest, Helmet, NOHelmet)\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def draw_text_and_box_on_image(image, text, position, box_coordinates, font, color):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    # Draw bounding box\n",
    "    draw.rectangle(box_coordinates, outline=color, width=2)\n",
    "    \n",
    "    # Draw text with specified color\n",
    "    draw.text(position, text, fill=color, font=font)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def process_webcam(model, timeout=5):\n",
    "    # Open the webcam\n",
    "    cap = cv2.VideoCapture(0)  # 0 corresponds to the default webcam\n",
    "\n",
    "    # Create a font with the desired size\n",
    "    font_size = 20\n",
    "    font = ImageFont.truetype(\"arial.ttf\", font_size)\n",
    "\n",
    "    # Initialize timeout variables\n",
    "    last_detection_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        # Read a frame from the webcam\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Convert the OpenCV BGR image to RGB (PIL format)\n",
    "        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        try:\n",
    "            results = model.predict(image)\n",
    "            result = results[0]\n",
    "\n",
    "            # Create a flag to track if detection occurred in the current frame\n",
    "            detection_occurred = False\n",
    "\n",
    "            for idx, box in enumerate(result.boxes):\n",
    "                class_id = result.names[box.cls[0].item()]\n",
    "                cords = box.xyxy[0].tolist()\n",
    "                cords = [round(x) for x in cords]\n",
    "                conf = round(box.conf[0].item(), 2)\n",
    "\n",
    "                # Determine color based on class_id\n",
    "                if class_id == \"Helmet\":\n",
    "                    bounding_box_color = (0, 255, 0)  # Green\n",
    "                    text_color = (0, 255, 0)  # Green\n",
    "                elif class_id == \"NOHelmet\":\n",
    "                    bounding_box_color = (0, 0, 255)  # Blue\n",
    "                    text_color = (0, 0, 255)  # Blue\n",
    "                elif class_id == \"NOVest\":\n",
    "                    bounding_box_color = (255, 0, 0)  # Red\n",
    "                    text_color = (255, 0, 0)  # Red\n",
    "                elif class_id == \"Vest\":\n",
    "                    bounding_box_color = (255, 255, 0)  # Yellow\n",
    "                    text_color = (255, 255, 0)  # Yellow\n",
    "                else:\n",
    "                    bounding_box_color = (128, 128, 128)  # Default to gray\n",
    "                    text_color = (128, 128, 128)  # Default to gray\n",
    "\n",
    "                # Draw text and bounding box on the image\n",
    "                text = f\"{class_id}({conf})\"\n",
    "                position = (cords[0], cords[1] - 22)  # Adjust the position based on your preference\n",
    "                image_with_text_and_box = draw_text_and_box_on_image(image, text, position, cords, font, text_color)\n",
    "\n",
    "                # Convert the modified image back to BGR (OpenCV format)\n",
    "                modified_frame = cv2.cvtColor(np.array(image_with_text_and_box), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                # Set the flag to True if at least one detection occurs\n",
    "                detection_occurred = True\n",
    "\n",
    "                # Update the last detection time\n",
    "                last_detection_time = time.time()\n",
    "\n",
    "            # Show the modified frame on the screen only if detection occurred\n",
    "            if detection_occurred:\n",
    "                cv2.imshow('Webcam Detection', modified_frame)\n",
    "            else:\n",
    "                # Show the original frame when no detection occurs\n",
    "                cv2.imshow('Webcam Detection', frame)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing frame: {e}\")\n",
    "\n",
    "        # Break the loop when 'q' key is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the webcam\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "model = YOLO(\"Your Model path\")\n",
    "process_webcam(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to apply on video and get output of safety/unsafety percentages for Helmet and Vest detection.\n",
    "\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "def apply_safety_detection(video):\n",
    "    # Code to apply safety detection model on the video\n",
    "    model = YOLO(\"Your Model path\")\n",
    "\n",
    "    # Open the video capture\n",
    "    cap = cv2.VideoCapture(str(video))\n",
    "    \n",
    "    # Get the frames per second (fps) of the input video\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    frame_number = 0\n",
    "\n",
    "    # Initialize counts for safety and unsafety\n",
    "    safety_count = 0\n",
    "    unsafety_count = 0\n",
    "\n",
    "    while True:\n",
    "        # Read a frame from the video\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Break the loop if the video has ended\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            results = model.predict(frame)  # Assuming the model can directly predict on OpenCV frame\n",
    "            result = results[0]\n",
    "\n",
    "            # Update counts for safety and unsafety based on detections\n",
    "            safety_classes = [\"Vest\", \"Helmet\"]  # Adjust this list based on your model's class names\n",
    "            for idx, box in enumerate(result.boxes):\n",
    "                class_id = result.names[box.cls[0].item()]\n",
    "                conf = box.conf[0].item()\n",
    "\n",
    "                if class_id in safety_classes:  # and conf >= 0.5\n",
    "                    safety_count += 1\n",
    "                else:\n",
    "                    unsafety_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing frame {frame_number}: {e}\")\n",
    "\n",
    "        frame_number += 1\n",
    "\n",
    "    # Release the video capture\n",
    "    cap.release()\n",
    "    \n",
    "    # Calculate percentages based on the total number of detections\n",
    "    total_detections = safety_count + unsafety_count\n",
    "\n",
    "    # Avoid division by zero\n",
    "    safety_percentage = (safety_count / total_detections) * 100 if total_detections > 0 else 0\n",
    "    unsafety_percentage = (unsafety_count / total_detections) * 100 if total_detections > 0 else 0\n",
    "\n",
    "    return str(video), safety_percentage, unsafety_percentage\n",
    "\n",
    "apply_safety_detection(\"Your Input video path\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Person detection in Restricted area\n",
    "\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "\n",
    "model = YOLO(\"Yout Model path\")\n",
    "cap = cv2.VideoCapture(\"Your Input video path\")\n",
    "\n",
    "exit_delay_counter = 0\n",
    "person_inside_polygon = False\n",
    "processed_persons = []\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "desired_delay_seconds = 0.3\n",
    "delay_frames = int(fps * desired_delay_seconds)\n",
    "\n",
    "def is_point_inside_polygon(point, polygon):\n",
    "    if len(polygon) >= 2:\n",
    "        result = cv2.pointPolygonTest(polygon, point, False)\n",
    "        return result > 0\n",
    "    return False\n",
    "\n",
    "drawing = False\n",
    "polygon_points = []\n",
    "\n",
    "def draw_polygon(event, x, y, flags, param):\n",
    "    global drawing, polygon_points\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        drawing = True\n",
    "        polygon_points.append((x, y))\n",
    "    elif event == cv2.EVENT_LBUTTONUP and drawing:\n",
    "        polygon_points.append((x, y))\n",
    "        poly = cv2.polylines(frame, [np.array(polygon_points)], isClosed=True, color=(0, 0, 255), thickness=2)\n",
    "\n",
    "    elif event == cv2.EVENT_RBUTTONDOWN and drawing:\n",
    "        drawing = False\n",
    "        cv2.polylines(frame, [np.array(polygon_points)], isClosed=True, color=(0, 0, 255), thickness=2)\n",
    "        polygon_points = []\n",
    "\n",
    "cv2.namedWindow('video')\n",
    "cv2.setMouseCallback('video', draw_polygon)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.resize(frame, (640, 480))\n",
    "\n",
    "    result = model.predict(frame, classes=[0])\n",
    "    frame = result[-1].plot(conf=False)\n",
    "\n",
    "    new_person_inside_polygon = False\n",
    "\n",
    "    for box in result[-1].boxes.data:\n",
    "        class_id = int(box[5])\n",
    "        if class_id == 0:\n",
    "            person_bbox = box[:4].cpu().numpy()\n",
    "\n",
    "            x, y, w, h = person_bbox\n",
    "            cv2.rectangle(frame, tuple(map(int, person_bbox[:2])), tuple(map(int, person_bbox[2:])), (0, 255, 0), 2)\n",
    "\n",
    "            person_center_x = (x + w) / 2\n",
    "            person_center_y = (y + h) / 2\n",
    "\n",
    "            center = (int(person_center_x), int(person_center_y))\n",
    "            cv2.circle(frame, center, 5, (0, 255, 255), -1)\n",
    "\n",
    "            top_right_corner = (person_bbox[2], person_bbox[1])\n",
    "            top_left_corner = (person_bbox[0], person_bbox[1])\n",
    "            bottom_left_corner = (person_bbox[0], person_bbox[3])\n",
    "            bottom_right_corner = (person_bbox[2], person_bbox[3])\n",
    "\n",
    "            intersection_area = is_point_inside_polygon(bottom_left_corner, np.array(polygon_points)) or is_point_inside_polygon(bottom_right_corner, np.array(polygon_points)) or is_point_inside_polygon(top_left_corner, np.array(polygon_points)) or is_point_inside_polygon(top_right_corner, np.array(polygon_points)) or is_point_inside_polygon(center, np.array(polygon_points))\n",
    "\n",
    "            if intersection_area and person_bbox.tolist() not in processed_persons:\n",
    "                cv2.rectangle(frame, tuple(map(int, person_bbox[:2])), tuple(map(int, person_bbox[2:])), (255, 0, 0), 2)\n",
    "                new_person_inside_polygon = True\n",
    "                processed_persons.append(person_bbox.tolist())\n",
    "\n",
    "    if not person_inside_polygon and new_person_inside_polygon:\n",
    "        person_inside_polygon = True\n",
    "        new_person_inside_polygon = False\n",
    "\n",
    "    elif person_inside_polygon and not new_person_inside_polygon:\n",
    "        exit_delay_counter += 1\n",
    "        if exit_delay_counter >= delay_frames:\n",
    "            person_inside_polygon = False\n",
    "            exit_delay_counter = 0\n",
    "\n",
    "    if person_inside_polygon:\n",
    "        cv2.polylines(frame, [np.array(polygon_points)], isClosed=True, color=(0, 0, 255), thickness=2)\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(frame, 'person entered restricted area', (10, 30), font, 0.8, (255, 255, 255), 2)\n",
    "    else:\n",
    "        cv2.polylines(frame, [np.array(polygon_points)], isClosed=True, color=(255, 255, 255), thickness=2)\n",
    "\n",
    "    cv2.imshow(\"video\", frame)\n",
    "    if cv2.waitKey(1) == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
